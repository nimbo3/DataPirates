politeness.waiting.time=30
hbase.table.name=sites
hbase.table.column.family.anchors=links
hbase.bulk.size=500
langDetect.profile.dir=profiles
fetcher.threads.num=500
processor.threads.num=50
fetcher.connection.timeout.milliseconds=30000
# check max redirects!!
fetcher.max.redirects=2
crawlerThread.registry.name=crawler
metric.registry.name=data-pirates-crawler
metric.name.hbase.bulk=hbase-bulk-insert
metric.name.kafka.shutdown=kafka-shutdown
metric.name.shutdown=shutdown
metric.name.parser=parser
metric.name.linkProducer=kafka-sending
metric.name.linkConsumer=kafka-receiving
metric.registry.timer.name=app initializing
metric.domain.name=crawler
metric.name.redis.visit.check=redis-visited-check
kafka.consume.poll.timeout=100
fetcher.client.num.of.maximum.total.connections=500
fetcher.client.num.of.maximum.connections.per.route=1
elastic.bulk.timeout=30
elastic.insertion.metric.name=elastic-insertion
elastic.insertion.failure.metric.name=elastic-insertion-failure
elastic.delete.metric.name=elastic-delete
elastic.hostname=slave2
elastic.port=9200
elastic.bulk.size=1000
elastic.bulk.flush.interval.seconds=15
elastic.concurrent.requests=5
elastic.backoff.delay.seconds=3
elastic.backoff.retries=2
kafka.bootstrap.servers=slave1:9092,slave2:9092,slave3:9092
kafka.group.id=tester
kafka.enable.auto.commit=true
kafka.auto.commit.interval.ms=1000
kafka.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.acks=all
kafka.key.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.topic.name=links
kafka.buffer.size=1000
redis.servers=slave1:7000,slave2:7000,slave3:7000
metric.registery.name=data-pirates-crawler